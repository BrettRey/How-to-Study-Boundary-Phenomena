---
title: "How to Study Boundary Phenomena"
subtitle: "English Reciprocals and the Limits of Categorization"
author: "Brett Reynolds"
institute: "Humber Polytechnic & University of Toronto"
date: "10 April 2026"
format:
  revealjs:
    theme: [default, custom.scss]
    width: 1600
    height: 900
    margin: 0.05
    center: false
    hash: true
    navigationMode: linear
    controls: false
    progress: true
    slideNumber: true
    embed-resources: true
    fig-align: center
    slide-level: 2
---

## What makes a category real?

::: {.incremental}

- [Traditional answer:]{.term} necessary and sufficient conditions
- [Problem:]{.term} linguistic categories resist definition. Pronouns share properties, but no single property is shared by all and only pronouns.
- [Alternative:]{.term} a [robust cluster]{.term} of co-occurring properties + [stabilizing mechanisms]{.term} that maintain the cluster = a real category

:::

. . .

[*Homeostatic* = self-correcting, like body temperature: the system drifts, mechanisms push it back. A category maintained this way is [projectible]{.term} -- you can make predictions about new instances.]{.thesis}

::: {.notes}
~2 min. Open with the problem that motivates the whole talk. Traditional categories need necessary and sufficient conditions, but grammatical categories don't have those. HPC theory comes from philosophy of biology (Boyd 1991, 1999) -- it's how biologists think about species. Miller (2021) brought it to linguistics. The name: *homeostatic* means self-correcting, like body temperature -- the system has mechanisms that push it back when it drifts. *Property cluster* = the category is defined by a cluster of co-occurring properties, not by necessary and sufficient conditions. If the audience looks blank at *homeostatic*, the body-temperature analogy lands well. The spinning top slide will reinforce this. Key term to land: *projectible* -- you can make predictions about new instances. Delivery tip (Harris): consider opening with the particular (*each other*) before the general (HPC theory) -- the theory may land better after the puzzle is on the table. Even a one-sentence teaser works.
:::


## No real kind without a purpose

:::: {.columns}

::: {.column width="48%"}
[[Syntactician's proper noun]{.term}]{style="color: #8A6A28;"}

- Distribution: *Brett left* / \**The Brett left*
- Agreement: 3rd person singular
- Modification: restricted
- [Typically has [proper name]{.term} semantics]{style="color: #9B6B9E; font-style: italic;"}
:::

::: {.column width="48%"}
[[Semanticist's proper name]{.term}]{style="color: #9B6B9E;"}

- Rigid designation
- Referential opacity
- Sense vs. reference
- [Usually instantiated by a [proper noun]{.term}]{style="color: #8A6A28; font-style: italic;"}
:::

::::

[*Brett* is both. Different projections, different mechanisms, different HPCs, same extension.]{.thesis}

::: {.notes}
~1.5 min. Concrete example of why the HPC framework matters. The proper noun / proper name distinction is familiar to this audience but the projection point is the key: knowing that *Brett* is a proper name (semantic fact) doesn't help you predict its syntactic distribution. The two categories are maintained by different mechanisms and project differently. The cross-coloured items show that the clusters overlap (proper nouns typically have proper name semantics, proper names are typically realized by proper nouns) but the overlap is contingent, not definitional. *The Hague* is a proper name but not a bare proper noun.
:::


## Homeostasis: the virtuous circle

What holds these clusters together?

:::: {.circle-diagram}

::: {.circle-node}
Property cluster<br>
<span style="font-size: 0.9em; font-style: italic;">co-occurring properties</span>
:::

::: {.circle-arrows}
stabilizes &rarr;

&larr; sustains
:::

::: {.circle-node}
Mechanisms<br>
<span style="font-size: 0.9em; font-style: italic;">causal processes</span>
:::

::::

::: {.mechanism-list}
[Five grammatical mechanisms:]{.term} morphological realization rules &middot; agreement/case systems &middot; entrenched distributional patterns &middot; grammaticalization pathways &middot; community norms
:::

<br>

Mechanisms maintain clusters. Clusters maintain mechanisms. That's what [homeostatic]{.term} means. (A reciprocal relationship, as it happens.)

::: {.notes}
~1.5 min. The core theoretical machinery. Bridge from slide 3: we saw that different mechanisms produce different kinds. Now: what's the relationship between mechanisms and properties? Properties cluster because mechanisms maintain them. But the cluster itself sustains the mechanisms (speakers learn and reinforce the patterns because the cluster exists). This reciprocal reinforcement is what makes HPC categories real and stable. That's the "homeostatic" in HPC -- self-correcting, like body temperature. The five mechanisms listed are from the paper's footnote 2, adapted from Miller (2021). We'll come back to this diagram at the end when we see which mechanisms are pulling reciprocals in which direction.
:::


## Stability is dynamic, not static

![](plots/top-vs-ball.png){fig-align="center" height="450"}

[Grammatical categories are spinning tops, not balls in valleys.]{.thesis}

::: {.notes}
~1.5 min. Now that we've seen what homeostasis is (the virtuous circle), show what kind of stability it produces. Ball in valley = passive equilibrium, the kind you get from definitions. Spinning top = dynamic stability, the kind you get from mechanisms actively maintaining the category. Push a spinning top slightly and gyroscopic forces bring it back. Stop the spin and the top falls. Stop the mechanisms and the category dissolves. This is why "what maintains the category" matters more than "what defines it."
:::


## The reciprocals puzzle

Our question is [syntactic]{.term}: what [lexical category]{.term} do *each other* and *one another* belong to? [Pronoun]{.term} or [compound determinative]{.term}?

[Determinative]{.term}: the word class of *the*, *some*, *every* — not the [determiner]{.term} function. [Compound determinatives]{.term} like *somebody* (*some* + *body*) and *everyone* (*every* + *one*) share the compound structure of *each other* (*each* + *other*).

| | Pronoun-like | Determinative-like |
|---|---|---|
| Morphology (66) | | Compound; not monomorphemic; no distinct accusative, genitive, or reflexive forms |
| Semantics (36) | Pro-form (stands in for a full NP); definite; anaphoric; requires an antecedent | |
| Syntax (50) | Not in partitives (\**each other of the people*); not in existentials; doesn't take *else* | Accepts genitive *'s*; appears in object |
| Phonology (3) | | (weak signal) |

155 properties. Morphology pulls one way, semantics the other, syntax is mixed. Which way do they go?

::: {.notes}
~2 min. Start by landing the pivot: we're asking a syntactic question, so morphological and semantic facts are evidence, not the answer itself. Pullum's keynote is about category/function confusions, so this audience will appreciate the precision. Quick terminology: determinative is a word class (category), determiner is a function — the distinction Pullum is keynoting on. Compound determinatives like *somebody*, *everyone*, *nothing* have compound morphology and serve as fused determiner–head in NP: one word does both jobs. Reciprocals have the same compound structure (*each* + *other*), which looks determinative-like. But semantically they're pro-forms, definite, anaphoric — they require an antecedent that structurally outranks them, like pronouns. Morphologically they lack the distinct accusative, genitive, and reflexive forms that pronouns like *he/him/his/himself* have — more like compound determinatives. Syntactically it's mixed: they don't function as fused determiner–head (pronoun-like), and they don't appear in existentials or take *else* (pronoun-like), but they accept genitive *'s* and appear in object position (like *somebody*). The numbers in parentheses show how many features per block — these are just highlights from 155 total. Agreement is 3rd person either way, so it's not diagnostic. This is the kind of problem where cherry-picking is easy and tempting.
:::


## The problem with cherry-picking

Two items, 155 tests, and a strong temptation to cherry-pick.

. . .

Croft (2001) calls this [methodological opportunism]{.term}: consciously or not, we select tests that support our preferred analysis.

. . .

The alternative: measure the [stability of diagnostic ambiguity]{.term}. Vary every reasonable analytic choice and ask whether the answer changes.

. . .

[The interesting question isn't "which category?" but "how stable is the apparent boundary position under different measurement choices?"]{.thesis}

::: {.notes}
~1 min. Brief slide. Croft's "methodological opportunism" names the pathology: settling on an answer because it's congenial rather than because inquiry compels it. The alternative is measuring stability instead of picking convenient diagnostics. If a questioner raises Peirce, you can connect this to his "method of tenacity" (from "The Fixation of Belief," 1877) and his alternative method of science: let reality resist your expectations.
:::


## What HPC predicts for boundary items

- [Stable position]{.term}: the result doesn't depend on how you measure
- [Cross-dimensional tension]{.term}: morphology and semantics pull in different directions
- [Clean anchors]{.term}: clear cases come out right, so the method is trustworthy
- [Near-parity mixture]{.term}: the item sits right at the midpoint between the two categories
- [Robustness to null]{.term}: scramble the data keeping its basic structure; the pattern shouldn't appear by chance — and it doesn't

These aren't arbitrary desiderata. They're consequences of the theory.

::: {.notes}
~1.5 min. Show all five at once so the audience sees the full structure, then walk through each. Each expectation follows from HPC theory: if categories are maintained by distinct mechanisms sustaining overlapping bundles, then boundary items should show exactly these properties. E2 is the distinctive HPC prediction: cross-dimensional tension (different feature families pulling different directions) is what you get when distinct mechanisms sustain partly conflicting bundles. Make sure this point lands.
:::


## Mapping grammatical space

155 binary properties (morphology, syntax, semantics, phonology) across 138 items. This 2D projection captures ~17% of the variance; all actual measurement uses full 155-dimensional Jaccard distances.

![Multiple Correspondence Analysis projection. Pronouns (blue) and determinatives (red) form regions; compound determinatives sit at the interface; reciprocals (triangles) fall in that interface zone.](plots/mca_reciprocals.png){fig-align="center" height="480"}

::: {.notes}
~1.5 min. The star figure. Like mapping vowel space: 155 features are the acoustic dimensions, pronouns and determinatives are the reference categories. MCA is just for intuition (like an F1–F2 plot). All actual measurement uses full 155-dimensional Jaccard distances. Point out the compound determinatives at the interface and the reciprocals right there with them. Don't dwell on MCA technicalities.
:::


## Not a statistical fluke

Scramble the data 5,000 times, preserving how many properties each word has and how many words have each property. This tests whether the *specific combination* of features drives reciprocals' position, not just marginal structure.

Observed pattern in only 0.6% of scrambles (*p* = 0.006).

![Permutation null distribution. Dashed line marks the observed value.](plots/permutation_null.png){fig-align="center" height="420"}

::: {.notes}
~1 min. Quick slide. The quasiswap preserves how many features each word has and how many words have each feature, so we're asking whether the specific combination of features in reciprocals drives their position. 0.6% says yes: not a fluke of marginal structure. But note: this is for the prespecified comparison set. Sensitivity to comparator choice is addressed next.
:::


## Stable across analytic choices

Vary every reasonable analytic choice (distance metric, which properties, weighting) and show all results. Each point is one specification; Delta = mean distance to pronouns minus mean distance to determinatives.

![Four metrics (Jaccard, Dice, Hamming, IDF-weighted) &times; two regularizations (ridge, elastic net). Positive = closer to determinatives; negative = closer to pronouns. Exception: removing morphology flips the sign.](plots/spec_curve.png){fig-align="center" height="420"}

Sign stable across most choices. Removing morphology flips it. That's [cross-dimensional tension]{.term}.

::: {.notes}
~1.5 min. The specification curve is the garden-of-forking-paths analysis. Vary distance metric (Jaccard, Dice, IDF-weighted), feature set (ablate each block), and regularization. Most specs land on the same side of zero. But removing morphology flips the sign: without morphological features, reciprocals look more pronoun-like. This is exactly the cross-dimensional tension HPC predicts (E2): morphology pulls toward determinative, semantics pulls toward pronoun.
:::


## Right at the midpoint

Best-fitting mixture weight: *each other* = 0.534, *one another* = 0.487

![Every item sorted from determinative (0) to pronoun (1). Reciprocals sit at the midpoint.](plots/eta_strip.pdf){fig-align="center" height="450"}

::: {.notes}
~1 min. The mixture calibration asks: if reciprocals are a blend of the two anchor profiles, what blend best predicts their observed properties? Answer: almost exactly 50/50. These mixture weights (0.534, 0.487) are SSE-minimizing blends from the calibration curve, not posterior means. Separately, a classifier assigns near-chance pronoun probabilities (0.485 for *each other*, 0.467 for *one another*), and the predictive log-likelihood is identical under either labelling (-54.240). Two independent methods, same verdict: the model genuinely can't tell. Both weights are stable under modest feature ablations and alternative scoring rules (spec curve, previous slide).
:::


## All five expectations confirmed

| | Expectation | Result |
|:---:|---|---|
| &#10004; | [Stable position]{.term} | Result stable no matter how you measure |
| &#10004; | [Cross-dimensional tension]{.term} | Morphology &rarr; determinative; semantics &rarr; pronoun |
| &#10004; | [Clean anchors]{.term} | Same methods correctly identify clear cases |
| &#10004; | [Near-parity mixture]{.term} | Best-fitting weights 0.534, 0.487 (near midpoint) |
| &#10004; | [Robustness to null]{.term} | Pattern in only 0.6% of scrambled data |

<br>

[This isn't measurement failure. It's what a real boundary looks like.]{.thesis}

::: {.notes}
~1 min. Summary slide. All five HPC-derived expectations are met. Emphasize that these aren't post hoc rationalizations: they were derived from the theory before the analysis. The punchline: stable ambiguity is data, not noise.
:::


## What kind of problem is this?

Reciprocals *are* one or the other. But our instruments can't resolve which.

::: {style="position: relative; width: fit-content; margin: 0 auto;"}
![](plots/diffraction_limit.png){height="300"}

[Resolved]{style="position: absolute; top: 10px; left: 25%; transform: translateX(-50%); color: white; font-size: 0.8em;"}
[Unresolved]{style="position: absolute; top: 10px; left: 75%; transform: translateX(-50%); color: white; font-size: 0.8em;"}
:::

. . .

[Categories are internally gradient but sharply bounded. This isn't gradience; it's a boundary phenomenon: independent mechanisms sustaining opposed pulls.]{.thesis}

::: {.notes}
~1.5 min. The slide has two beats: the diffraction image, then the thesis sentence. Open with the image: reciprocals are one category or the other, but our diagnostics can't resolve which -- just like two points below the diffraction limit. Let the image sit for a moment before clicking through. Then land the thesis: this isn't gradience (smooth fading of membership across a single dimension). Gradience predicts uniform weakening across all features. What we see is the opposite -- morphology pulls one way, semantics the other, each stably. That's the signature of distinct mechanisms maintaining overlapping bundles. The boundary is ontologically sharp but epistemically inaccessible. Spare material for Q&A: (1) Peirce's scholastic realism -- categories are real, not convenient fictions; fallibilism -- our knowledge of them is always provisional. (2) The empirical pattern is compatible with Slater's Stable Property Clusters and Khalidi's causal-network account. What distinguishes HPC is the emphasis on cross-dimensional tension as the signature of distinct mechanisms. (3) If boundaries were sharp but arbitrarily located, we'd expect greater sensitivity to metric choice than the specification curve reveals.
:::


## Back to the spinning top

:::: {.circle-diagram}

::: {.circle-node}
Property cluster<br>
<span style="font-size: 0.9em; font-style: italic;">co-occurring properties</span>
:::

::: {.circle-arrows}
stabilizes &rarr;

&larr; sustains
:::

::: {.circle-node}
Mechanisms<br>
<span style="font-size: 0.9em; font-style: italic;">causal processes</span>
:::

::::

<br>

:::: {.columns}

::: {.column width="50%"}
[Determinative region]{.term} (morphology pull)

Morphological realization rules maintain compound structure: *each* + *other*
:::

::: {.column width="50%"}
[Pronoun region]{.term} (semantics pull)

Interpretive mechanisms (reference tracking, anaphoric dependencies) maintain pronoun-like behaviour
:::

::::

<br>

Two sets of mechanisms, two spinning tops, one boundary. Stop either spin and the tension dissolves.

::: {.notes}
~1.5 min. Bring it full circle. The virtuous circle from slide 4 now has content: the determinative region is maintained by morphological realization rules (compound structure), the pronoun region by interpretive mechanisms (reference tracking, binding). Reciprocals sit where these two sets of mechanisms overlap and conflict. That's why the cross-dimensional tension exists and why it's stable. If asked about the stabilizes/sustains dynamic, use the footpath analogy. A footpath across a field: the cluster is the observable bundle of properties (worn ground, short grass, compacted soil, drainage follows it). The mechanism is repeated foot traffic. Foot traffic produces and maintains all those properties as a bundle. The visible path (the cluster) attracts more walkers (sustains the mechanism). Take away the walkers and the properties stop co-occurring: grass grows back, soil loosens, drainage shifts. They were bundled because something kept them bundled. For reciprocals: the pronoun-like cluster is the observable bundle (anaphoric, requires an antecedent, definite, pro-form). The mechanism is reference tracking in the grammar. Reference tracking keeps those properties bundled. Because the bundle exists as a stable, learnable pattern, new speakers acquire it, which perpetuates the reference-tracking behaviour. The cluster is what you can see. The mechanism is why it holds together. The loop is that what holds together is what gets learned.
:::


## How to study boundary phenomena

1. Build comprehensive profiles (don't cherry-pick diagnostics)
2. Test against scrambled baselines (especially with small *n*)
3. Vary specifications systematically (show all results)
4. Calibrate against clear cases (verify known structure)
5. Ask whether the ambiguity is [stable]{.term}

<br>

[We asked what makes a category real. The [HPC]{.term} answer: maintenance. [Stable ambiguity]{.term} is evidence of that maintenance at work.]{.thesis}

<br>

Paper: [LingBuzz 009294](https://ling.auf.net/lingbuzz/009294) &middot; Code: [GitHub](https://github.com/BrettRey/How-to-Study-Boundary-Phenomena) &middot; [brettreynolds.ca](https://brettreynolds.ca) &middot; brett.reynolds@humber.ca

::: {.notes}
This slide stays up during Q&A. The practical checklist from the paper's conclusion. Emphasize that this transfers: any small-n categorization problem (modal auxiliaries, discourse particles, anything) can use the same toolkit. The insight is simple: stop forcing binary decisions, measure stability instead.
:::
